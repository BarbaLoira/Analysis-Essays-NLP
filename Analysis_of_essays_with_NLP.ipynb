{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analysis of essays with NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarbaLoira/Analysis-Essays-NLP/blob/master/Analysis_of_essays_with_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xp6irXHBn9O8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Install uol-redacoes-xml"
      ]
    },
    {
      "metadata": {
        "id": "gWx11T9doFEY",
        "colab_type": "code",
        "outputId": "04ad2432-3e6b-4b6f-f155-fc451762a906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/gpassero/uol-redacoes-xml.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/gpassero/uol-redacoes-xml.git\n",
            "  Cloning https://github.com/gpassero/uol-redacoes-xml.git to /tmp/pip-req-build-wvsr5de_\n",
            "Building wheels for collected packages: uol-redacoes-xml\n",
            "  Building wheel for uol-redacoes-xml (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-f4ynx0tt/wheels/1b/d6/69/d465fd205ecd2058fe620852fb31bdf5cd0b9697ec2d7548eb\n",
            "Successfully built uol-redacoes-xml\n",
            "Installing collected packages: uol-redacoes-xml\n",
            "Successfully installed uol-redacoes-xml-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R1MrzKAgoGYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#import and load uol_redacoes_xml"
      ]
    },
    {
      "metadata": {
        "id": "7-37iRG8ojjA",
        "colab_type": "code",
        "outputId": "3e9ffb53-3a81-4a3c-cfd6-a934ae958f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import uol_redacoes_xml\n",
        "\n",
        "essays = uol_redacoes_xml.load()\n",
        "# print(len(essays)) #~2000\n",
        "\n",
        "# print(essays[0].text) #texto original da primeira redação\n",
        "\n",
        "# print(  [attr for attr in essays[0].__dir__() if not attr.startswith('_')]   ) # exibe os atributos do objeto de redação (exceto os privados, que começam com '_')\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "UOL essays load warnings: \n",
            "No text  ->  3\n",
            "Final score != from sum of criteria score  ->  45\n",
            "Not 5 criteria  ->  7\n",
            "Total warnings: 55\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AISDcT21pwUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Imports Useful"
      ]
    },
    {
      "metadata": {
        "id": "i-96vfVRqqz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "\n",
        "from uol_redacoes_xml.reader.commons import kfold_cross_validation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "#!python -m spacy download pt\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "\n",
        "import pyphen\n",
        "#!pip install pyphen\n",
        "#dic = pyphen.Pyphen(lang='pt')\n",
        "#dic.inserted('macaco')\n",
        "pyphenPt = pyphen.Pyphen(lang='pt')\n",
        "\n",
        "#!pip install textstat\n",
        "from textstat.textstat import legacy_round \n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oUIEEojyda_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#formatando o texto"
      ]
    },
    {
      "metadata": {
        "id": "Qi-aIQ1ydeeQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "variables  = ['title', 'text' , 'final_score' ]\n",
        "data = [ [getattr(i,attr) for attr in essays[0].__dir__() if not attr.startswith('_') and not attr.startswith('prompt') and not attr.startswith('url') and not attr.startswith('fixed_text') and not attr.startswith('errors') and not attr.startswith('get_features') and not attr.startswith('comments') and not attr.startswith('criteria_scores') ] for i in essays]\n",
        "\n",
        "text_formatted = pd.DataFrame(data ,columns = variables )\n",
        "text_formatted\n",
        "#text_formatted.head()\n",
        "#text_formatted.tail()\n",
        "#text_formatted[\"text\"][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96QmJTXEWuF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Flesch Reading Ease"
      ]
    },
    {
      "metadata": {
        "id": "o_S5hjdoWzSm",
        "colab_type": "code",
        "outputId": "ccdd3e3a-7afa-456c-9f26-67750d323d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "text = text_formatted[\"text\"][2]\n",
        "flesch_reading_ease('O Doutor Armando foi até a esquina esperá o filho que chegava da escola. Nisso, a Maria ficou em casa preparando o almoço.Quando eles chegarão em casa a Maria tava na cozinha preparando a famosa receita da família boa pra caramba o bolo de fubá cremoso.')\n",
        "\n",
        "countWords = len(count_words_text(text))\n",
        "\n",
        "countSentence = len(list(nlp(text_formatted[\"text\"][2]).sents))\n",
        "  \n",
        "\n",
        "countSylable = len((pyphenPt.inserted(text_formatted[\"text\"][2])).split(\"-\"))\n",
        "\n",
        "\n",
        "206.836 - ((1.105 * (countWords/countSentence)) - (84.6 * (countSylable / countWords)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54.22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "VceOotH4HC0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Util"
      ]
    },
    {
      "metadata": {
        "id": "3_fj_eOtHKN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def break_sentences(text):   \n",
        "    doc = nlp(text)\n",
        "    return list(doc.sents) \n",
        "  \n",
        "# Returns Number of Words in the text \n",
        "def word_count(text): \n",
        "    sentences = break_sentences(text) \n",
        "    words = 0\n",
        "    for sentence in sentences: \n",
        "        words += len([token for token in sentence]) \n",
        "    return words \n",
        "  \n",
        "# Returns the number of sentences in the text \n",
        "def sentence_count(text): \n",
        "    sentences = break_sentences(text) \n",
        "    return len(sentences)  \n",
        "  \n",
        "  \n",
        "  # Returns average sentence length \n",
        "def avg_sentence_length(text): \n",
        "    words = word_count(text) \n",
        "    sentences = sentence_count(text) \n",
        "    average_sentence_length = float(words / sentences) \n",
        "    return average_sentence_length \n",
        "  \n",
        "def syllables_count(word): \n",
        "    return len((pyphenPt.inserted(word)).split(\"-\"))\n",
        "  \n",
        "  \n",
        "  # Returns the average number of syllables per \n",
        "# word in the text \n",
        "def avg_syllables_per_word(text): \n",
        "    syllable = syllables_count(text) \n",
        "    words = word_count(text) \n",
        "    ASPW = float(syllable) / float(words) \n",
        "    return legacy_round(ASPW, 1) \n",
        "  \n",
        "  \n",
        "# Return total Difficult Words in a text \n",
        "def difficult_words(text): \n",
        "  \n",
        "    # Find all words in the text \n",
        "    words = [] \n",
        "    sentences = break_sentences(text) \n",
        "    for sentence in sentences: \n",
        "        words += [str(token) for token in sentence] \n",
        "  \n",
        "    # difficult words are those with syllables >= 2 \n",
        "    # easy_word_set is provide by Textstat as  \n",
        "    # a list of common words \n",
        "    diff_words_set = set() \n",
        "      \n",
        "    for word in words: \n",
        "        syllable_count = syllables_count(word) \n",
        "        if word not in easy_word_set and syllable_count >= 2: \n",
        "            diff_words_set.add(word) \n",
        "  \n",
        "    return len(diff_words_set) \n",
        "  \n",
        "# A word is polysyllablic if it has more than 3 syllables \n",
        "# this functions returns the number of all such words  \n",
        "# present in the text \n",
        "def poly_syllable_count(text): \n",
        "    count = 0\n",
        "    words = [] \n",
        "    sentences = break_sentences(text) \n",
        "    for sentence in sentences: \n",
        "        words += [token for token in sentence] \n",
        "      \n",
        "  \n",
        "    for word in words: \n",
        "        syllable_count = syllables_count(word) \n",
        "        if syllable_count >= 3: \n",
        "            count += 1\n",
        "    return count \n",
        "  \n",
        "  # muito facil        75 a 100     (primeiros 4 anos escolares)\n",
        "  # facil              50 a 75      (quita a oitava serie)\n",
        "  # pouco dificil      25 a 50      (colegial e o nivel superior)\n",
        "  # muito dificil   abaixo de 25    (texto academico) \n",
        "def flesch_reading_ease(text): \n",
        "    \"\"\" \n",
        "        Implements Flesch Formula: \n",
        "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n",
        "        Here, \n",
        "          ASL = average sentence length (number of words  \n",
        "                divided by number of sentences) \n",
        "          ASW = average word length in syllables (number of syllables  \n",
        "                divided by number of words) \n",
        "    \"\"\"\n",
        "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) - float(84.6 * avg_syllables_per_word(text)) \n",
        "    return legacy_round(FRE, 2)   \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}